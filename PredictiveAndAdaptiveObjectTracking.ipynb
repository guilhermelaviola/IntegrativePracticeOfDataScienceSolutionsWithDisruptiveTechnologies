{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgo7IPeaLnE2eRjnZIbwrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegrativePracticeOfDataScienceSolutionsWithDisruptiveTechnologies/blob/main/PredictiveAndAdaptiveObjectTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PredictiveAndAdaptiveObjectTracking.ipynb**\n",
        "Object tracking in videos presents significant challenges due to changing perspectives and unpredictable appearance variations, as exemplified in a bustling retail environment like Target during a sale. Traditional tracking methods, reliant on correlation techniques and manual feature extraction, face limitations under adverse conditions, such as sudden angle shifts or lighting changes. The advent of deep learning, particularly regression networks, marks a pivotal advancement by enabling the prediction of object locations in future frames. These networks learn to anticipate complex movements, allowing for accurate tracking even amid rapid changes, like customers swiftly moving items or abrupt shifts in camera angles. This predictive and adaptive capability enhances tracking robustness and efficiency, demonstrating its critical value in active scenarios, especially within high-traffic retail settings.â€‹"
      ],
      "metadata": {
        "id": "utoTU5dfRRBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_D_T0lsIRS6a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w3B3bWLQ6US"
      },
      "outputs": [],
      "source": [
        "# Loading a video simulating a busy retail environment:\n",
        "video_path = 'store_simulation.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initializing tracker (SiamMask requires PyTorch and pretrained weights)\n",
        "# For demonstration, we will use CSRT which has good accuracy and some robustness:\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "\n",
        "# Reading the first frame:\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    print('Failed to read video. Please ensure the video file exists at the specified path.')\n",
        "    # It's better to return or raise an exception in a notebook to prevent further errors\n",
        "    # rather than just calling exit(), which might not stop cell execution.\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    # You might want to upload 'store_simulation.mp4' to your Colab environment.\n",
        "else:\n",
        "    # Defining initial bounding box of the object to track (e.g., a product or a shopping cart)\n",
        "    # In a real scenario, this can come from a detector (YOLO / Faster R-CNN):\n",
        "    bbox = cv2.selectROI('Frame', frame, False)\n",
        "    tracker.init(frame, bbox)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Updating tracker (predict the next position of the object):\n",
        "        success, bbox = tracker.update(frame)\n",
        "\n",
        "        if success:\n",
        "            # Tracker successfully predicts object location:\n",
        "            x, y, w, h = [int(v) for v in bbox]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, 'Tracking', (x, y-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "        else:\n",
        "            # Tracker lost the object due to occlusion / abrupt motion\n",
        "            # Here is where predictive capabilities are crucial\n",
        "            # Advanced deep trackers would predict next location based on trajectory:\n",
        "            cv2.putText(frame, 'Predicting...', (50,50),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
        "\n",
        "        cv2.imshow('Frame', frame)\n",
        "        key = cv2.waitKey(30)\n",
        "        if key == 27:  # ESC to quit\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ]
    }
  ]
}