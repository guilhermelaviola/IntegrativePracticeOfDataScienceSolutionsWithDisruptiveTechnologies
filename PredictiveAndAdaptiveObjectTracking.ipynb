{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpPd5/U07a1sY55zP4aAlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegrativePracticeOfDataScienceSolutionsWithDisruptiveTechnologies/blob/main/PredictiveAndAdaptiveObjectTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PredictiveAndAdaptiveObjectTracking.ipynb**\n",
        "Object tracking in videos presents significant challenges due to changing perspectives and unpredictable appearance variations, as exemplified in a bustling retail environment like Target during a sale. Traditional tracking methods, reliant on correlation techniques and manual feature extraction, face limitations under adverse conditions, such as sudden angle shifts or lighting changes. The advent of deep learning, particularly regression networks, marks a pivotal advancement by enabling the prediction of object locations in future frames. These networks learn to anticipate complex movements, allowing for accurate tracking even amid rapid changes, like customers swiftly moving items or abrupt shifts in camera angles. This predictive and adaptive capability enhances tracking robustness and efficiency, demonstrating its critical value in active scenarios, especially within high-traffic retail settings.â€‹"
      ],
      "metadata": {
        "id": "utoTU5dfRRBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_D_T0lsIRS6a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "_w3B3bWLQ6US",
        "outputId": "ba8740b4-fd16-4660-de31-82bf288ce797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to read video\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2968013798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Defining initial bounding box of the object to track (e.g., a product or a shopping cart)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# In a real scenario, this can come from a detector (YOLO / Faster R-CNN):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectROI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Frame\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
          ]
        }
      ],
      "source": [
        "# Loading a video simulating a busy retail environment:\n",
        "video_path = 'store_simulation.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initializing tracker (SiamMask requires PyTorch and pretrained weights)\n",
        "# For demonstration, we will use CSRT which has good accuracy and some robustness:\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "\n",
        "# Reading the first frame:\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    print('Failed to read video')\n",
        "    exit()\n",
        "\n",
        "# Defining initial bounding box of the object to track (e.g., a product or a shopping cart)\n",
        "# In a real scenario, this can come from a detector (YOLO / Faster R-CNN):\n",
        "bbox = cv2.selectROI(\"Frame\", frame, False)\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Updating tracker (predict the next position of the object):\n",
        "    success, bbox = tracker.update(frame)\n",
        "\n",
        "    if success:\n",
        "        # Tracker successfully predicts object location:\n",
        "        x, y, w, h = [int(v) for v in bbox]\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, 'Tracking', (x, y-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "    else:\n",
        "        # Tracker lost the object due to occlusion / abrupt motion\n",
        "        # Here is where predictive capabilities are crucial\n",
        "        # Advanced deep trackers would predict next location based on trajectory:\n",
        "        cv2.putText(frame, 'Predicting...', (50,50),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
        "\n",
        "    cv2.imshow('Frame', frame)\n",
        "    key = cv2.waitKey(30)\n",
        "    if key == 27:  # ESC to quit\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}