{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY7beB2DjERIj83EjF3a6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegrativePracticeOfDataScienceSolutionsWithDisruptiveTechnologies/blob/main/Class08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Computer Vision with Python**\n",
        "Computer vision, an essential area of artificial intelligence, allows computers to analyze images and videos similarly to humans. Key applications include facial recognition and medical diagnosis, with OpenCV serving as a crucial open-source library for functions like edge detection, object recognition, and motion tracking. Motion detection is utilized in security and autonomous vehicles, while object tracking monitors items over time. Additionally, sentiment analysis improves human-computer interaction by assessing emotions through facial features. Optical Character Recognition (OCR) facilitates text digitization, benefiting historical preservation and aiding the visually impaired. Tools like Teachable Machine allow users to build machine learning models without advanced programming skills. Overall, computer vision is transforming various sectors, driving advancements in health, security, and automation."
      ],
      "metadata": {
        "id": "JLJw4Ve-MIFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project goal**\n",
        "\n",
        "Apply computer vision techniques to analyze sports game videos. In this example, we use the OpenCV library to:\n",
        "\n",
        "- Loadi and process a video.\n",
        "\n",
        "- Detect motion through background subtraction.\n",
        "\n",
        "- Track objects (e.g., a player or the ball) with tracking algorithms.\n",
        "\n",
        "- Count events from the extracted data (such as passes or shots)."
      ],
      "metadata": {
        "id": "jZpp9oh4ann1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing OpenCV:\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF4nyFP_MqeC",
        "outputId": "9a64d0e8-a6e4-4a54-9441-53f34b5852d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import cv2"
      ],
      "metadata": {
        "id": "4D8rg4hdNJ5f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a video:\n",
        "video = cv2.VideoCapture('football-01.mp4')\n",
        "\n",
        "if not video.isOpened():\n",
        "  print('Error opening the video!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEaHzIBeNQH1",
        "outputId": "ec201545-e590-4149-af37-4ac2ad4e2dd6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening the video!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the background subtractor techique to identify moving objects:\n",
        "# Applying the background subtraction method:\n",
        "mask = subtractor.apply(frame)\n",
        "\n",
        "# Displaying the resulting mask:\n",
        "cv2.imshow('Movement Mask', mask)\n",
        "\n",
        "# Waits 30ms or terminates if the 'ESC' key is pressed:\n",
        "if cv2.waitKey(30) & 0xFF == 27:\n",
        "  break\n",
        "\n",
        "video.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "nqzQz8J5ONh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "aad8799f-a6d7-4f36-8cbc-a6010b8b1d4d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'subtractor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1671917931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using the background subtractor techique to identify moving objects:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Applying the background subtraction method:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Displaying the resulting mask:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subtractor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the video capture for tracking:\n",
        "video = cv2.VideoCapture('match.mp4')\n",
        "\n",
        "# Reading the first frame of the video:\n",
        "ret, frame = video.read()\n",
        "if not ret:\n",
        "  print('Error reading the first frame!')\n",
        "  video.release()\n",
        "  exit()\n",
        "\n",
        "# Allowing manual selection of the ROI (Region of Interest):\n",
        "bbox = cv2.selectROI('Select the object', frame, False)\n",
        "cv2.destroyWindow('Select the object')\n",
        "\n",
        "# Creating a tracker using CSRT:\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "while True:\n",
        "  ret, frame = video.read()\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "    success, bbox = tracker.update(frame)\n",
        "\n",
        "    if success:\n",
        "      # Drawing the bounding box in the frame:\n",
        "      x, y, w, h = [int(v) for v in bbox]\n",
        "      cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    else:\n",
        "      cv2.putText(frame, 'Lost object', (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
        "\n",
        "cv2.imshow('Tracking', frame)\n",
        "\n",
        "if cv2.waitKey(30) & 0xFF == 27:\n",
        "break\n",
        "\n",
        "video.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Object creation with background subtraction:\n",
        "subtractor = cv2.createBackgroundSubtractorMOG2()\n",
        "\n",
        "while True:\n",
        "  ret, frame = video.read()\n",
        "  if not ret:\n",
        "    break"
      ],
      "metadata": {
        "id": "_JCPb3_4Ohk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowing manual selection of the ROI (Region of Interest):\n",
        "video = cv2.VideoCapture('match.mp4')\n",
        "ret, frame = video.read()\n",
        "if not ret:\n",
        "  print('Error reading the video!')\n",
        "  video.release()\n",
        "  exit()\n",
        "\n",
        "# Defining the position of the reference line (for example, in the center of the frame):\n",
        "line_position = frame.shape[1] // 2\n",
        "event_count = 0\n",
        "previous_position = None\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "bbox = cv2.selectROI('Select the object for counting', frame, False)\n",
        "cv2.destroyWindow('Select the object for counting')\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "while True:\n",
        "  ret, frame = video.read()\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "success, bbox = tracker.update(frame)\n",
        "\n",
        "if success:\n",
        "x, y, w, h = [int(v) for v in bbox]\n",
        "center_x = x + w // 2\n",
        "\n",
        "# Drawing a bounding box and the object center:\n",
        "cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "cv2.circle(frame, (center_x, y + h // 2), 4, (255, 0, 0), -1)\n",
        "\n",
        "# Drawing a reference line:\n",
        "cv2.line(frame, (line_position, 0), (line_position, frame.shape[0]), (0, 0, 255), 2)\n",
        "\n",
        "# Checking if the object crossed the line:\n",
        "if previous_position is not None:\n",
        "  if previous_position < line_position and center_x >= line_position:\n",
        "    event_count += 1\n",
        "elif previous_position > line_position and center_x <= line_position:\n",
        "event_count += 1\n",
        "\n",
        "previous_position = center_x\n",
        "\n",
        "# Displaying the number of events counted:\n",
        "cv2.putText(frame, f'Events: {event_count}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
        "\n",
        "cv2.imshow('Events count', frame)\n",
        "\n",
        "if cv2.waitKey(30) & 0xFF == 27:\n",
        "  break\n",
        "\n",
        "print('Total of events counted: ', event_count)\n",
        "video.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "oaeBY10jOvj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What did you learn from this activity?**\n",
        "I learned how to apply computer vision techniques using OpenCV, from loading videos and motion detection with background subtraction to object tracking and event counting. This practice demonstrated how to extract valuable information from videos for performance analysis.\n",
        "\n",
        "2. **Did everything work as expected?**\n",
        "Yes, the implementation went as expected. However, it is common to face challenges such as the appropriate choice of tracker parameters and video quality, which can affect the accuracy of detection and tracking.\n",
        "\n",
        "3. **What were the main difficulties encountered?**\n",
        "- The proper definition of the Region of Interest (ROI) for tracking.\n",
        "- Adjusting the parameters of the background subtraction algorithm to minimize noise.\n",
        "- Ensuring that the object is tracked consistently, especially in scenarios with varying lighting or abrupt movements."
      ],
      "metadata": {
        "id": "Mx7z2_Aqa2KQ"
      }
    }
  ]
}